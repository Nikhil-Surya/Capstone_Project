{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba84adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Set your API key\n",
    "api_key = 'API KEY'\n",
    "os.environ['OPENAI_API_KEY'] = api_key\n",
    "\n",
    "# System and Human message templates\n",
    "template = \"\"\"You are an expert assistant providing accurate, honest, and detailed information. Ensure that your answers are:\n",
    "- Factually correct and well-supported.\n",
    "- Clear and easy to understand.\n",
    "Always aim for an informative and objective response.\"\"\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "# Combine both into a chat prompt\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# Initialize the chain\n",
    "chain = LLMChain(\n",
    "    llm=ChatOpenAI(openai_api_key=api_key, model_name='gpt-4' ),\n",
    "    prompt=chat_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a04101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read prompts from a text file \n",
    "def read_prompts_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        return [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# Dataset\n",
    "prompt_file = 'input.txt'\n",
    "prompts_list = read_prompts_from_file(prompt_file)\n",
    "\n",
    "# Prepare data storage for CSV/Excel\n",
    "output_data = []\n",
    "\n",
    "# Running the chain and generating responses\n",
    "for prompt in prompts_list:\n",
    "    # Generate prediction from the chain\n",
    "    prediction = chain.run(prompt)\n",
    "    \n",
    "    # Store the prompt and prediction in the output_data list\n",
    "    output_data.append({\n",
    "        'Prompt': prompt,\n",
    "        'Response': prediction.strip()\n",
    "    })\n",
    "\n",
    "    # Print the result to the console (optional)\n",
    "    print(f'\\nPROMPT: {prompt}')\n",
    "    print(f'RESPONSE: {prediction.strip()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
